---
title: '[2DUB] 크롤링 + TFIDF'
date: 2019-04-08
---

## 데이터 셋 만들고 전처리 하기

2dub 동영상과 가장 비슷한 데이터를 실습으로 이용하기 위해 10~15 문장 정도 구성되어 있는 유튜브 trailer에 있는 영어 자막을 크롤링 하여 예제 데이터 셋으로 사용합니다.

![image](https://user-images.githubusercontent.com/48308562/55700665-18e03e00-5a0b-11e9-8c25-582da94f845d.png)

크롤링 하기 전 유튜브 api를 이용하기 위해 key를 얻어야 하는데 [이곳](https://developers.google.com/youtube/registering_an_application?hl=ko)을 참고 하여 개인 key를 받습니다.

api key를 받았다면 다음 [bitbucket](https://bitbucket.org/donghyunPark/youtube_crawling/src/master/)에 있는 파이썬 파일들을 pull하거나 개인적으로 다운받아 사용합니다.

그리고 아래 그림처럼 영화의 URL주소와 API key를 입력하면 (pymysql, sqlite 선택 가능) 자신의 데이터 베이스에 information과 caption 테이블이 생성되며 영화 정보와 자막이 따로 저장됩니다. (저는 조커라는 영화를 다운받기 전에 7개의 영화를 미리 저장 해놓은 상태이기에 데이터가 더 많이 보입니다. 데이터 베이스는 먼저 아무 이름으로 생성 해주세요. 저는 youtube라는 이름의 db를 먼저 생성했습니다.)

![image](https://user-images.githubusercontent.com/48308562/55701187-2e566780-5a0d-11e9-9c34-449ee59c9aa7.png)

![image](https://user-images.githubusercontent.com/48308562/55701407-f4d22c00-5a0d-11e9-93cd-c22bf40ae732.png)

![image](https://user-images.githubusercontent.com/48308562/55701304-9a38d000-5a0d-11e9-9cda-7853c354ac07.png)

위 사진과 같이 자막(caption) 데이터를 보시면 '[]' 안에 Music(Applause, ...)같은 필요없는 데이터가 들어가 있는 것을 보실 수 있습니다. 자막 데이터를 충분히 다운로드 받으셨다면 SQL 쿼리로 전처리를 해줍니다.

*대괄호로 시작해서 대괄호로 끝나는 caption 행 삭제*

`delete from caption where caption like '[%%]'`

![image](https://user-images.githubusercontent.com/48308562/55702541-dbcb7a00-5a11-11e9-8320-ce2d42bba18a.png)


## TFIDF

#### 데이터 불러오기

데이터베이스에 존재하는 자막들을 파이썬으로 불러와 TFIDF를 통해 각 문장의 단어별로 점수를 부여합니다. 코드를 보면서 알아봅시다.

```python
import re
from operator import itemgetter
import pymysql.cursors
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 데이터 불러오기

conn = pymysql.connect(host='localhost',
                         user='root',
                         password='111111',
                         db = 'youtube',
                         charset='utf8mb4')
cursor = conn.cursor()

# ==== select example ====
sql = "select caption from caption"
cursor.execute(sql)

# 데이타 Fetch
captions = cursor.fetchall()
cursor.close()

captions = list(captions) # change from tuple to list type
captions
```

#### 전처리

아래와 같은 전처리 과정은 나중에 데이터 셋이 변경될 때 추가적으로 전처리 과정을 추가하거나 변경해 줍니다.

```python
# '[Music, Applause, ...]' <- 전처리 and remove tuple
def preprocessing(caption):
    caption = re.sub("[^a-zA-Z]", " ", str(caption)).strip()
    words = caption.lower().split()
    clean_caption = ' '.join(words)
    return clean_caption

clean_captions = []

for caption in captions:
    clean_captions.append(preprocessing(caption))

# TfidfVectorizer 모듈을 이용해 모든 단어를 dictionary화(numbering) 해주고 tfidf 점수를 구합니다.

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(clean_captions)

# 어떤 단어에 몇번 째 number가 할당 되었는지 확인
print(tfidf_vectorizer.vocabulary_)    

{'have': 216, 'never': 333, 'had': 210, 'feeling': 151, 'as': 28, 'pure': 381, 'proud': 379, 'completing': 97, 'mission': 315, 'all': 13, 'you': 571, 'everything': 142, 'we': 539,... 생략}
```

그러면 아래 그림처럼 tf-idf를 자동으로 계산해주고 tfidf_matrix에 저장합니다. 이때 tfidf_matrix는 그냥은 확인할 수 없고 cosine 계산시에 할당을 해줄 수 있습니다.

![image](https://user-images.githubusercontent.com/48308562/55703520-ca37a180-5a14-11e9-8fa4-e6a6a74e75df.png)


#### cosine_similarity로 유사도를 구한 후 top 5 문장 추출하기

그리고 단어 유사도를 구하기 위해 각도의 개념도 포함이 되어 가장 성능이 좋은 Cosine-Similarity를 사용하여 첫번째 단어와 다른 모든 단어들 간의 Similarity를 구한 후 TOP 5개의 문장을 추출 해 봅니다.


```python
# extract top 5

checked_similarity = []

for i in range(len(clean_captions)):

    if cosine_similarity(tfidf_matrix[10], tfidf_matrix[i]) != 0:

        checked_similarity.append([captions[i][0], cosine_similarity(tfidf_matrix[10], tfidf_matrix[i])])  

checked_similarity = sorted(checked_similarity, key=itemgetter(1), reverse=True)
top_5 = checked_similarity[1:6]  
print("Input sentence: " + captions[10][0])
print("\nSimilar sentences: ")
top_5      
```
